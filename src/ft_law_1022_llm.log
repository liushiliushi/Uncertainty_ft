nohup: ignoring input
W1023 04:27:47.740863 140219395401536 torch/distributed/run.py:779] 
W1023 04:27:47.740863 140219395401536 torch/distributed/run.py:779] *****************************************
W1023 04:27:47.740863 140219395401536 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1023 04:27:47.740863 140219395401536 torch/distributed/run.py:779] *****************************************
/home/lyb/workspace/Uncertainty_ft/src/llama_recipes/model_checkpointing/checkpoint_handler.py:31: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead
  import torch.distributed._shard.checkpoint as dist_cp
/home/lyb/workspace/Uncertainty_ft/src/llama_recipes/model_checkpointing/checkpoint_handler.py:31: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead
  import torch.distributed._shard.checkpoint as dist_cp
Clearing GPU cache for all ranks
--> Running with torch dist debug set to detail
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:00,  6.71it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:00<00:00,  7.06it/s]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:00<00:00,  7.25it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  7.09it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  7.08it/s]
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
wandb: Currently logged in as: liushiliushi0 (liushiliushi0-lyb). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.8
wandb: Run data is saved locally in /home/lyb/workspace/Uncertainty_ft/src/wandb/run-20241023_042753-2qkvpegi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run drawn-waterfall-656
wandb: â­ï¸ View project at https://wandb.ai/liushiliushi0-lyb/llama_recipes
wandb: ðŸš€ View run at https://wandb.ai/liushiliushi0-lyb/llama_recipes/runs/2qkvpegi
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:00,  4.98it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:00<00:00,  5.62it/s]--> applying fsdp activation checkpointing...
Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:00<00:00,  5.91it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  6.57it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  6.17it/s]
--> Model /home/lyb/workspace/meta-llama/Llama-3.1-8B-Instruct

--> /home/lyb/workspace/meta-llama/Llama-3.1-8B-Instruct has 8030.261248 Million params

trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
bFloat16 enabled for mixed precision - using bfSixteen policy
--> Num of Validation Set Batches loaded = 42
Current learning rate: 0.0001
/home/uncertainty2/lib/python3.9/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
Training Epoch: 1:   0%|[34m          [0m| 0/1 [00:00<?, ?it/s]--> applying fsdp activation checkpointing...
--> Training Set Length = 10
--> Validation Set Length = 340
--> Num of Validation Set Batches loaded = 42
Current learning rate: 0.0001
/home/uncertainty2/lib/python3.9/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
Training Epoch: 1:   0%|[34m          [0m| 0/1 [00:00<?, ?it/s]
label: 1  1  1  0
prob: 80  80  80  80
loss: 0.21819420158863068

label: 0  0  1  0
prob: 100  75  0  80
loss: 0.5347644090652466
/home/uncertainty2/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/home/uncertainty2/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/home/uncertainty2/lib/python3.9/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]
/home/uncertainty2/lib/python3.9/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]
Training Epoch: 1: 100%|[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m| 1/1 [00:04<00:00,  4.20s/it]Training Epoch: 1/3, step 0/1 completed (loss: 0.5347644090652466): 100%|[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m| 1/1 [00:04<00:00,  4.20s/it]Training Epoch: 1: 100%|[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m| 1/1 [00:10<00:00, 10.04s/it]Training Epoch: 1/3, step 0/1 completed (loss: 0.21819420158863068): 100%|[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m| 1/1 [00:10<00:00, 10.04s/it]Max CUDA memory allocated was 12 GB
Max CUDA memory reserved was 14 GB
Peak active CUDA memory was 12 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 1 GB

evaluating Epoch:   0%|[32m          [0m| 0/42 [00:00<?, ?it/s][A
evaluating Epoch:   0%|[32m          [0m| 0/42 [00:00<?, ?it/s][Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

evaluating Epoch:   2%|[32mâ–         [0m| 1/42 [00:01<00:53,  1.30s/it][A
evaluating Epoch:   2%|[32mâ–         [0m| 1/42 [00:01<00:54,  1.32s/it][A
evaluating Epoch:   5%|[32mâ–         [0m| 2/42 [00:02<00:43,  1.08s/it][A
evaluating Epoch:   5%|[32mâ–         [0m| 2/42 [00:02<00:43,  1.08s/it][A
evaluating Epoch:   7%|[32mâ–‹         [0m| 3/42 [00:03<00:39,  1.01s/it][A
evaluating Epoch:   7%|[32mâ–‹         [0m| 3/42 [00:03<00:39,  1.01s/it][A
evaluating Epoch:  10%|[32mâ–‰         [0m| 4/42 [00:04<00:37,  1.02it/s][A
evaluating Epoch:  10%|[32mâ–‰         [0m| 4/42 [00:04<00:37,  1.02it/s][A
evaluating Epoch:  12%|[32mâ–ˆâ–        [0m| 5/42 [00:05<00:35,  1.03it/s][A
evaluating Epoch:  12%|[32mâ–ˆâ–        [0m| 5/42 [00:05<00:36,  1.02it/s][A
evaluating Epoch:  14%|[32mâ–ˆâ–        [0m| 6/42 [00:05<00:34,  1.04it/s][A
evaluating Epoch:  14%|[32mâ–ˆâ–        [0m| 6/42 [00:05<00:34,  1.04it/s][A
evaluating Epoch:  17%|[32mâ–ˆâ–‹        [0m| 7/42 [00:06<00:33,  1.05it/s][A
evaluating Epoch:  17%|[32mâ–ˆâ–‹        [0m| 7/42 [00:06<00:33,  1.05it/s][A
evaluating Epoch:  19%|[32mâ–ˆâ–‰        [0m| 8/42 [00:07<00:32,  1.05it/s][A
evaluating Epoch:  19%|[32mâ–ˆâ–‰        [0m| 8/42 [00:07<00:32,  1.05it/s][A
evaluating Epoch:  21%|[32mâ–ˆâ–ˆâ–       [0m| 9/42 [00:08<00:31,  1.05it/s][A
evaluating Epoch:  21%|[32mâ–ˆâ–ˆâ–       [0m| 9/42 [00:08<00:31,  1.05it/s][A
evaluating Epoch:  24%|[32mâ–ˆâ–ˆâ–       [0m| 10/42 [00:09<00:29,  1.07it/s][A
evaluating Epoch:  24%|[32mâ–ˆâ–ˆâ–       [0m| 10/42 [00:09<00:29,  1.07it/s][A
evaluating Epoch:  26%|[32mâ–ˆâ–ˆâ–Œ       [0m| 11/42 [00:10<00:29,  1.07it/s][A
evaluating Epoch:  26%|[32mâ–ˆâ–ˆâ–Œ       [0m| 11/42 [00:10<00:29,  1.06it/s][A
evaluating Epoch:  29%|[32mâ–ˆâ–ˆâ–Š       [0m| 12/42 [00:11<00:27,  1.07it/s][A
evaluating Epoch:  29%|[32mâ–ˆâ–ˆâ–Š       [0m| 12/42 [00:11<00:27,  1.07it/s][A
evaluating Epoch:  31%|[32mâ–ˆâ–ˆâ–ˆ       [0m| 13/42 [00:12<00:27,  1.07it/s][A
evaluating Epoch:  31%|[32mâ–ˆâ–ˆâ–ˆ       [0m| 13/42 [00:12<00:27,  1.07it/s][A
evaluating Epoch:  33%|[32mâ–ˆâ–ˆâ–ˆâ–Ž      [0m| 14/42 [00:13<00:26,  1.07it/s][A
evaluating Epoch:  33%|[32mâ–ˆâ–ˆâ–ˆâ–Ž      [0m| 14/42 [00:13<00:26,  1.07it/s][A
evaluating Epoch:  36%|[32mâ–ˆâ–ˆâ–ˆâ–Œ      [0m| 15/42 [00:14<00:25,  1.07it/s][A
evaluating Epoch:  36%|[32mâ–ˆâ–ˆâ–ˆâ–Œ      [0m| 15/42 [00:14<00:25,  1.07it/s][A
evaluating Epoch:  38%|[32mâ–ˆâ–ˆâ–ˆâ–Š      [0m| 16/42 [00:15<00:24,  1.08it/s][A
evaluating Epoch:  38%|[32mâ–ˆâ–ˆâ–ˆâ–Š      [0m| 16/42 [00:15<00:24,  1.08it/s][A
evaluating Epoch:  40%|[32mâ–ˆâ–ˆâ–ˆâ–ˆ      [0m| 17/42 [00:16<00:23,  1.08it/s][A
evaluating Epoch:  40%|[32mâ–ˆâ–ˆâ–ˆâ–ˆ      [0m| 17/42 [00:16<00:23,  1.08it/s][A
evaluating Epoch:  43%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž     [0m| 18/42 [00:17<00:22,  1.09it/s][A
evaluating Epoch:  43%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž     [0m| 18/42 [00:17<00:22,  1.08it/s][A
evaluating Epoch:  45%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     [0m| 19/42 [00:18<00:21,  1.08it/s][A
evaluating Epoch:  45%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     [0m| 19/42 [00:18<00:21,  1.08it/s][A
evaluating Epoch:  48%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     [0m| 20/42 [00:18<00:20,  1.09it/s][A
evaluating Epoch:  48%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     [0m| 20/42 [00:18<00:20,  1.09it/s][A

evaluating Epoch:  50%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     [0m| 21/42 [00:19<00:19,  1.08it/s][Aevaluating Epoch:  50%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     [0m| 21/42 [00:19<00:19,  1.08it/s][A
evaluating Epoch:  52%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    [0m| 22/42 [00:20<00:18,  1.08it/s][A
evaluating Epoch:  52%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    [0m| 22/42 [00:20<00:18,  1.08it/s][A
evaluating Epoch:  55%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    [0m| 23/42 [00:21<00:17,  1.08it/s][A
evaluating Epoch:  55%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    [0m| 23/42 [00:21<00:17,  1.08it/s][A
evaluating Epoch:  57%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    [0m| 24/42 [00:22<00:16,  1.08it/s][A
evaluating Epoch:  57%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    [0m| 24/42 [00:22<00:16,  1.08it/s][A
evaluating Epoch:  60%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    [0m| 25/42 [00:23<00:15,  1.08it/s][A
evaluating Epoch:  60%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    [0m| 25/42 [00:23<00:15,  1.08it/s][A
evaluating Epoch:  62%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   [0m| 26/42 [00:24<00:14,  1.07it/s][A
evaluating Epoch:  62%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   [0m| 26/42 [00:24<00:15,  1.06it/s][A
evaluating Epoch:  64%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   [0m| 27/42 [00:25<00:14,  1.07it/s][A
evaluating Epoch:  64%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   [0m| 27/42 [00:25<00:14,  1.06it/s][A
evaluating Epoch:  67%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   [0m| 28/42 [00:26<00:13,  1.07it/s][A
evaluating Epoch:  67%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   [0m| 28/42 [00:26<00:13,  1.07it/s][A

evaluating Epoch:  69%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   [0m| 29/42 [00:27<00:12,  1.07it/s][Aevaluating Epoch:  69%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   [0m| 29/42 [00:27<00:12,  1.07it/s][A
evaluating Epoch:  71%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  [0m| 30/42 [00:28<00:11,  1.07it/s][A
evaluating Epoch:  71%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  [0m| 30/42 [00:28<00:11,  1.07it/s][A
evaluating Epoch:  74%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  [0m| 31/42 [00:29<00:10,  1.07it/s][A
evaluating Epoch:  74%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  [0m| 31/42 [00:29<00:10,  1.07it/s][A
evaluating Epoch:  76%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  [0m| 32/42 [00:30<00:09,  1.07it/s][A
evaluating Epoch:  76%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  [0m| 32/42 [00:30<00:09,  1.07it/s][A
evaluating Epoch:  79%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  [0m| 33/42 [00:31<00:08,  1.08it/s][A
evaluating Epoch:  79%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  [0m| 33/42 [00:31<00:08,  1.08it/s][A
evaluating Epoch:  81%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  [0m| 34/42 [00:32<00:07,  1.07it/s][A
evaluating Epoch:  81%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  [0m| 34/42 [00:32<00:07,  1.07it/s][A
evaluating Epoch:  83%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž [0m| 35/42 [00:32<00:06,  1.07it/s][A
evaluating Epoch:  83%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž [0m| 35/42 [00:33<00:06,  1.07it/s][A
evaluating Epoch:  86%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ [0m| 36/42 [00:33<00:05,  1.07it/s][A
evaluating Epoch:  86%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ [0m| 36/42 [00:33<00:05,  1.07it/s][A
evaluating Epoch:  88%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š [0m| 37/42 [00:34<00:04,  1.08it/s][A
evaluating Epoch:  88%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š [0m| 37/42 [00:34<00:04,  1.08it/s][A
evaluating Epoch:  90%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ [0m| 38/42 [00:35<00:03,  1.08it/s][A
evaluating Epoch:  90%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ [0m| 38/42 [00:35<00:03,  1.08it/s][A

evaluating Epoch:  93%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž[0m| 39/42 [00:36<00:02,  1.08it/s][Aevaluating Epoch:  93%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž[0m| 39/42 [00:36<00:02,  1.08it/s][A
evaluating Epoch:  95%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ[0m| 40/42 [00:37<00:01,  1.09it/s][A
evaluating Epoch:  95%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ[0m| 40/42 [00:37<00:01,  1.09it/s][A
evaluating Epoch:  98%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š[0m| 41/42 [00:38<00:00,  1.08it/s][A
evaluating Epoch:  98%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š[0m| 41/42 [00:38<00:00,  1.08it/s][A
evaluating Epoch: 100%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m| 42/42 [00:39<00:00,  1.08it/s][A
evaluating Epoch: 100%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m| 42/42 [00:39<00:00,  1.08it/s][Aevaluating Epoch: 100%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m| 42/42 [00:39<00:00,  1.06it/s]
evaluating Epoch: 100%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m| 42/42 [00:39<00:00,  1.06it/s]
 eval_ppl=tensor(1.3984, device='cuda:0') eval_epoch_loss=tensor(0.3353, device='cuda:0')
we are about to save the PEFT modules
FullyShardedDataParallel(
  (_fsdp_wrapped_module): PeftModelForCausalLM(
    (base_model): LoraModel(
      (model): LlamaForCausalLM(
        (model): LlamaModel(
          (embed_tokens): Embedding(128256, 4096, padding_idx=128004)
          (layers): ModuleList(
            (0-31): 32 x FullyShardedDataParallel(
              (_fsdp_wrapped_module): CheckpointWrapper(
                (_checkpoint_wrapped_module): LlamaDecoderLayer(
                  (self_attn): LlamaSdpaAttention(
                    (q_proj): lora.Linear(
                      (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.05, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=4096, out_features=8, bias=False)
                        )
                      )
                      (lora_B): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=8, out_features=4096, bias=False)
                        )
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                      (lora_magnitude_vector): ModuleDict()
                    )
                    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
                    (v_proj): lora.Linear(
                      (base_layer): Linear(in_features=4096, out_features=1024, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.05, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=4096, out_features=8, bias=False)
                        )
                      )
                      (lora_B): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=8, out_features=1024, bias=False)
                        )
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                      (lora_magnitude_vector): ModuleDict()
                    )
                    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                    (rotary_emb): LlamaRotaryEmbedding()
                  )
                  (mlp): LlamaMLP(
                    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
                    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
                    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
                    (act_fn): SiLU()
                  )
                  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
                  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
                )
              )
            )
          )
          (norm): LlamaRMSNorm((4096,), eps=1e-05)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
      )
    )
  )
)
FullyShardedDataParallel(
  (_fsdp_wrapped_module): PeftModelForCausalLM(
    (base_model): LoraModel(
      (model): LlamaForCausalLM(
        (model): LlamaModel(
          (embed_tokens): Embedding(128256, 4096, padding_idx=128004)
          (layers): ModuleList(
            (0-31): 32 x FullyShardedDataParallel(
              (_fsdp_wrapped_module): CheckpointWrapper(
                (_checkpoint_wrapped_module): LlamaDecoderLayer(
                  (self_attn): LlamaSdpaAttention(
                    (q_proj): lora.Linear(
                      (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.05, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=4096, out_features=8, bias=False)
                        )
                      )
                      (lora_B): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=8, out_features=4096, bias=False)
                        )
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                      (lora_magnitude_vector): ModuleDict()
                    )
                    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
                    (v_proj): lora.Linear(
                      (base_layer): Linear(in_features=4096, out_features=1024, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.05, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=4096, out_features=8, bias=False)
                        )
                      )
                      (lora_B): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=8, out_features=1024, bias=False)
                        )
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                      (lora_magnitude_vector): ModuleDict()
                    )
                    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                    (rotary_emb): LlamaRotaryEmbedding()
                  )
                  (mlp): LlamaMLP(
                    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
                    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
                    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
                    (act_fn): SiLU()
                  )
                  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
                  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
                )
              )
            )
          )
          (norm): LlamaRMSNorm((4096,), eps=1e-05)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
      )
    )
  )
)
/home/uncertainty2/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/home/uncertainty2/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
PEFT modules are saved in checkpoints/professional1022 directory
best eval loss on epoch 1 is 0.33534225821495056
Current learning rate: 8.5e-05
Epoch 1: train_perplexity=1.4571, train_epoch_loss=0.3765, epoch time 4.576859377324581s
Current learning rate: 8.5e-05
/home/uncertainty2/lib/python3.9/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(

Training Epoch: 2:   0%|[34m          [0m| 0/1 [00:00<?, ?it/s][ATraining Epoch: 1/3, step 0/1 completed (loss: 0.21819420158863068): 100%|[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m| 1/1 [01:12<00:00, 72.47s/it]
/home/uncertainty2/lib/python3.9/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(

Training Epoch: 2:   0%|[34m          [0m| 0/1 [00:00<?, ?it/s][ATraining Epoch: 1/3, step 0/1 completed (loss: 0.5347644090652466): 100%|[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m| 1/1 [01:06<00:00, 66.65s/it]

label: 1  1  1  0
prob: 80  80  80  80
loss: 0.21523189544677734
/home/uncertainty2/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]

label: 0  0  1  0
prob: 0  80  0  80
loss: 0.4414580762386322
/home/uncertainty2/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/home/uncertainty2/lib/python3.9/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]
/home/uncertainty2/lib/python3.9/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]

Training Epoch: 2: 100%|[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m| 1/1 [00:03<00:00,  3.50s/it][A
Training Epoch: 2/3, step 0/1 completed (loss: 0.21523189544677734): 100%|[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m| 1/1 [00:03<00:00,  3.50s/it][A
Training Epoch: 2: 100%|[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m| 1/1 [00:03<00:00,  3.50s/it][A
Training Epoch: 2/3, step 0/1 completed (loss: 0.4414580762386322): 100%|[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m| 1/1 [00:03<00:00,  3.50s/it][AMax CUDA memory allocated was 12 GB
Max CUDA memory reserved was 14 GB
Peak active CUDA memory was 13 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 34 GB
evaluating Epoch:   0%|[32m          [0m| 0/42 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
evaluating Epoch:   0%|[32m          [0m| 0/42 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
evaluating Epoch:   2%|[32mâ–         [0m| 1/42 [00:01<00:44,  1.07s/it]evaluating Epoch:   2%|[32mâ–         [0m| 1/42 [00:01<00:45,  1.10s/it]evaluating Epoch:   5%|[32mâ–         [0m| 2/42 [00:01<00:39,  1.02it/s]evaluating Epoch:   5%|[32mâ–         [0m| 2/42 [00:02<00:39,  1.01it/s]evaluating Epoch:   7%|[32mâ–‹         [0m| 3/42 [00:02<00:37,  1.04it/s]evaluating Epoch:   7%|[32mâ–‹         [0m| 3/42 [00:02<00:37,  1.05it/s]evaluating Epoch:  10%|[32mâ–‰         [0m| 4/42 [00:03<00:36,  1.06it/s]evaluating Epoch:  10%|[32mâ–‰         [0m| 4/42 [00:03<00:36,  1.05it/s]evaluating Epoch:  12%|[32mâ–ˆâ–        [0m| 5/42 [00:04<00:35,  1.05it/s]evaluating Epoch:  12%|[32mâ–ˆâ–        [0m| 5/42 [00:04<00:35,  1.04it/s]evaluating Epoch:  14%|[32mâ–ˆâ–        [0m| 6/42 [00:05<00:33,  1.06it/s]evaluating Epoch:  14%|[32mâ–ˆâ–        [0m| 6/42 [00:05<00:34,  1.06it/s]evaluating Epoch:  17%|[32mâ–ˆâ–‹        [0m| 7/42 [00:06<00:32,  1.06it/s]evaluating Epoch:  17%|[32mâ–ˆâ–‹        [0m| 7/42 [00:06<00:33,  1.06it/s]evaluating Epoch:  19%|[32mâ–ˆâ–‰        [0m| 8/42 [00:07<00:32,  1.05it/s]evaluating Epoch:  19%|[32mâ–ˆâ–‰        [0m| 8/42 [00:07<00:32,  1.05it/s]evaluating Epoch:  21%|[32mâ–ˆâ–ˆâ–       [0m| 9/42 [00:08<00:31,  1.06it/s]evaluating Epoch:  21%|[32mâ–ˆâ–ˆâ–       [0m| 9/42 [00:08<00:31,  1.06it/s]evaluating Epoch:  24%|[32mâ–ˆâ–ˆâ–       [0m| 10/42 [00:09<00:29,  1.07it/s]evaluating Epoch:  24%|[32mâ–ˆâ–ˆâ–       [0m| 10/42 [00:09<00:29,  1.07it/s]evaluating Epoch:  26%|[32mâ–ˆâ–ˆâ–Œ       [0m| 11/42 [00:10<00:29,  1.07it/s]evaluating Epoch:  26%|[32mâ–ˆâ–ˆâ–Œ       [0m| 11/42 [00:10<00:29,  1.07it/s]evaluating Epoch:  29%|[32mâ–ˆâ–ˆâ–Š       [0m| 12/42 [00:11<00:27,  1.07it/s]evaluating Epoch:  29%|[32mâ–ˆâ–ˆâ–Š       [0m| 12/42 [00:11<00:27,  1.07it/s]evaluating Epoch:  31%|[32mâ–ˆâ–ˆâ–ˆ       [0m| 13/42 [00:12<00:27,  1.07it/s]evaluating Epoch:  31%|[32mâ–ˆâ–ˆâ–ˆ       [0m| 13/42 [00:12<00:27,  1.07it/s]evaluating Epoch:  33%|[32mâ–ˆâ–ˆâ–ˆâ–Ž      [0m| 14/42 [00:13<00:26,  1.07it/s]evaluating Epoch:  33%|[32mâ–ˆâ–ˆâ–ˆâ–Ž      [0m| 14/42 [00:13<00:26,  1.07it/s]evaluating Epoch:  36%|[32mâ–ˆâ–ˆâ–ˆâ–Œ      [0m| 15/42 [00:14<00:25,  1.07it/s]evaluating Epoch:  36%|[32mâ–ˆâ–ˆâ–ˆâ–Œ      [0m| 15/42 [00:14<00:25,  1.07it/s]evaluating Epoch:  38%|[32mâ–ˆâ–ˆâ–ˆâ–Š      [0m| 16/42 [00:15<00:24,  1.08it/s]evaluating Epoch:  38%|[32mâ–ˆâ–ˆâ–ˆâ–Š      [0m| 16/42 [00:15<00:24,  1.08it/s]evaluating Epoch:  40%|[32mâ–ˆâ–ˆâ–ˆâ–ˆ      [0m| 17/42 [00:15<00:23,  1.08it/s]evaluating Epoch:  40%|[32mâ–ˆâ–ˆâ–ˆâ–ˆ      [0m| 17/42 [00:16<00:23,  1.08it/s]evaluating Epoch:  43%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž     [0m| 18/42 [00:16<00:22,  1.09it/s]evaluating Epoch:  43%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž     [0m| 18/42 [00:16<00:22,  1.09it/s]evaluating Epoch:  45%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     [0m| 19/42 [00:17<00:21,  1.08it/s]evaluating Epoch:  45%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     [0m| 19/42 [00:17<00:21,  1.08it/s]evaluating Epoch:  48%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     [0m| 20/42 [00:18<00:20,  1.09it/s]evaluating Epoch:  48%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     [0m| 20/42 [00:18<00:20,  1.08it/s]evaluating Epoch:  50%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     [0m| 21/42 [00:19<00:19,  1.08it/s]evaluating Epoch:  50%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     [0m| 21/42 [00:19<00:19,  1.08it/s]evaluating Epoch:  52%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    [0m| 22/42 [00:20<00:18,  1.09it/s]evaluating Epoch:  52%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    [0m| 22/42 [00:20<00:18,  1.09it/s]evaluating Epoch:  55%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    [0m| 23/42 [00:21<00:17,  1.09it/s]evaluating Epoch:  55%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    [0m| 23/42 [00:21<00:17,  1.09it/s]evaluating Epoch:  57%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    [0m| 24/42 [00:22<00:16,  1.09it/s]evaluating Epoch:  57%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    [0m| 24/42 [00:22<00:16,  1.09it/s]evaluating Epoch:  60%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    [0m| 25/42 [00:23<00:15,  1.09it/s]evaluating Epoch:  60%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    [0m| 25/42 [00:23<00:15,  1.08it/s]evaluating Epoch:  62%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   [0m| 26/42 [00:24<00:14,  1.07it/s]evaluating Epoch:  62%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   [0m| 26/42 [00:24<00:15,  1.07it/s]evaluating Epoch:  64%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   [0m| 27/42 [00:25<00:13,  1.07it/s]evaluating Epoch:  64%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   [0m| 27/42 [00:25<00:14,  1.07it/s]evaluating Epoch:  67%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   [0m| 28/42 [00:26<00:13,  1.07it/s]evaluating Epoch:  67%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   [0m| 28/42 [00:26<00:13,  1.07it/s]evaluating Epoch:  69%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   [0m| 29/42 [00:27<00:12,  1.07it/s]evaluating Epoch:  69%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   [0m| 29/42 [00:27<00:12,  1.07it/s]evaluating Epoch:  71%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  [0m| 30/42 [00:28<00:11,  1.07it/s]evaluating Epoch:  71%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  [0m| 30/42 [00:28<00:11,  1.07it/s]evaluating Epoch:  74%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  [0m| 31/42 [00:29<00:10,  1.07it/s]evaluating Epoch:  74%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  [0m| 31/42 [00:28<00:10,  1.07it/s]evaluating Epoch:  76%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  [0m| 32/42 [00:29<00:09,  1.08it/s]evaluating Epoch:  76%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  [0m| 32/42 [00:29<00:09,  1.07it/s]evaluating Epoch:  79%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  [0m| 33/42 [00:30<00:08,  1.07it/s]evaluating Epoch:  79%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  [0m| 33/42 [00:30<00:08,  1.07it/s]evaluating Epoch:  81%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  [0m| 34/42 [00:31<00:07,  1.07it/s]evaluating Epoch:  81%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  [0m| 34/42 [00:31<00:07,  1.06it/s]evaluating Epoch:  83%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž [0m| 35/42 [00:32<00:06,  1.07it/s]evaluating Epoch:  83%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž [0m| 35/42 [00:32<00:06,  1.07it/s]evaluating Epoch:  86%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ [0m| 36/42 [00:33<00:05,  1.07it/s]evaluating Epoch:  86%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ [0m| 36/42 [00:33<00:05,  1.07it/s]evaluating Epoch:  88%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š [0m| 37/42 [00:34<00:04,  1.08it/s]evaluating Epoch:  88%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š [0m| 37/42 [00:34<00:04,  1.08it/s]evaluating Epoch:  90%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ [0m| 38/42 [00:35<00:03,  1.08it/s]evaluating Epoch:  90%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ [0m| 38/42 [00:35<00:03,  1.08it/s]evaluating Epoch:  93%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž[0m| 39/42 [00:36<00:02,  1.08it/s]evaluating Epoch:  93%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž[0m| 39/42 [00:36<00:02,  1.08it/s]evaluating Epoch:  95%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ[0m| 40/42 [00:37<00:01,  1.09it/s]evaluating Epoch:  95%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ[0m| 40/42 [00:37<00:01,  1.08it/s]evaluating Epoch:  98%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š[0m| 41/42 [00:38<00:00,  1.08it/s]evaluating Epoch:  98%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š[0m| 41/42 [00:38<00:00,  1.08it/s]evaluating Epoch: 100%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m| 42/42 [00:39<00:00,  1.08it/s]evaluating Epoch: 100%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m| 42/42 [00:39<00:00,  1.08it/s]evaluating Epoch: 100%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m| 42/42 [00:39<00:00,  1.07it/s]
evaluating Epoch: 100%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m| 42/42 [00:39<00:00,  1.07it/s]
 eval_ppl=tensor(1.3861, device='cuda:0') eval_epoch_loss=tensor(0.3265, device='cuda:0')
we are about to save the PEFT modules
FullyShardedDataParallel(
  (_fsdp_wrapped_module): PeftModelForCausalLM(
    (base_model): LoraModel(
      (model): LlamaForCausalLM(
        (model): LlamaModel(
          (embed_tokens): Embedding(128256, 4096, padding_idx=128004)
          (layers): ModuleList(
            (0-31): 32 x FullyShardedDataParallel(
              (_fsdp_wrapped_module): CheckpointWrapper(
                (_checkpoint_wrapped_module): LlamaDecoderLayer(
                  (self_attn): LlamaSdpaAttention(
                    (q_proj): lora.Linear(
                      (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.05, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=4096, out_features=8, bias=False)
                        )
                      )
                      (lora_B): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=8, out_features=4096, bias=False)
                        )
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                      (lora_magnitude_vector): ModuleDict()
                    )
                    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
                    (v_proj): lora.Linear(
                      (base_layer): Linear(in_features=4096, out_features=1024, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.05, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=4096, out_features=8, bias=False)
                        )
                      )
                      (lora_B): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=8, out_features=1024, bias=False)
                        )
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                      (lora_magnitude_vector): ModuleDict()
                    )
                    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                    (rotary_emb): LlamaRotaryEmbedding()
                  )
                  (mlp): LlamaMLP(
                    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
                    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
                    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
                    (act_fn): SiLU()
                  )
                  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
                  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
                )
              )
            )
          )
          (norm): LlamaRMSNorm((4096,), eps=1e-05)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
      )
    )
  )
)
FullyShardedDataParallel(
  (_fsdp_wrapped_module): PeftModelForCausalLM(
    (base_model): LoraModel(
      (model): LlamaForCausalLM(
        (model): LlamaModel(
          (embed_tokens): Embedding(128256, 4096, padding_idx=128004)
          (layers): ModuleList(
            (0-31): 32 x FullyShardedDataParallel(
              (_fsdp_wrapped_module): CheckpointWrapper(
                (_checkpoint_wrapped_module): LlamaDecoderLayer(
                  (self_attn): LlamaSdpaAttention(
                    (q_proj): lora.Linear(
                      (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.05, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=4096, out_features=8, bias=False)
                        )
                      )
                      (lora_B): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=8, out_features=4096, bias=False)
                        )
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                      (lora_magnitude_vector): ModuleDict()
                    )
                    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
                    (v_proj): lora.Linear(
                      (base_layer): Linear(in_features=4096, out_features=1024, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.05, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=4096, out_features=8, bias=False)
                        )
                      )
                      (lora_B): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=8, out_features=1024, bias=False)
                        )
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                      (lora_magnitude_vector): ModuleDict()
                    )
                    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                    (rotary_emb): LlamaRotaryEmbedding()
                  )
                  (mlp): LlamaMLP(
                    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
                    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
                    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
                    (act_fn): SiLU()
                  )
                  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
                  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
                )
              )
            )
          )
          (norm): LlamaRMSNorm((4096,), eps=1e-05)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
      )
    )
  )
)
/home/uncertainty2/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/home/uncertainty2/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
PEFT modules are saved in checkpoints/professional1022 directory
Current learning rate: 7.225000000000001e-05
best eval loss on epoch 2 is 0.32651835680007935
Epoch 2: train_perplexity=1.3887, train_epoch_loss=0.3283, epoch time 3.9019026160240173s
Current learning rate: 7.225000000000001e-05
/home/uncertainty2/lib/python3.9/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
/home/uncertainty2/lib/python3.9/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
Training Epoch: 3:   0%|[34m          [0m| 0/1 [00:00<?, ?it/s]Training Epoch: 2/3, step 0/1 completed (loss: 0.4414580762386322): 100%|[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m| 1/1 [01:05<00:00, 65.53s/it]
Training Epoch: 3:   0%|[34m          [0m| 0/1 [00:00<?, ?it/s]Training Epoch: 2/3, step 0/1 completed (loss: 0.21523189544677734): 100%|[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m| 1/1 [01:05<00:00, 65.55s/it]

label: 1  1  1  0
prob: 80  80  80  80
loss: 0.22340017557144165
/home/uncertainty2/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]

label: 0  0  1  0
prob: 0  75  0  80
loss: 0.3779709041118622
/home/uncertainty2/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/home/uncertainty2/lib/python3.9/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]
/home/uncertainty2/lib/python3.9/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]
Training Epoch: 3: 100%|[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m| 1/1 [00:03<00:00,  3.40s/it]Training Epoch: 3/3, step 0/1 completed (loss: 0.22340017557144165): 100%|[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m| 1/1 [00:03<00:00,  3.40s/it]Training Epoch: 3: 100%|[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m| 1/1 [00:03<00:00,  3.40s/it]Training Epoch: 3/3, step 0/1 completed (loss: 0.3779709041118622): 100%|[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m| 1/1 [00:03<00:00,  3.40s/it]Max CUDA memory allocated was 12 GB
Max CUDA memory reserved was 13 GB
Peak active CUDA memory was 12 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 34 GB

evaluating Epoch:   0%|[32m          [0m| 0/42 [00:00<?, ?it/s][A
evaluating Epoch:   0%|[32m          [0m| 0/42 [00:00<?, ?it/s][Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

evaluating Epoch:   2%|[32mâ–         [0m| 1/42 [00:01<00:44,  1.08s/it][A
evaluating Epoch:   2%|[32mâ–         [0m| 1/42 [00:01<00:45,  1.10s/it][A
evaluating Epoch:   5%|[32mâ–         [0m| 2/42 [00:01<00:39,  1.02it/s][A
evaluating Epoch:   5%|[32mâ–         [0m| 2/42 [00:02<00:39,  1.01it/s][A
evaluating Epoch:   7%|[32mâ–‹         [0m| 3/42 [00:02<00:37,  1.04it/s][A
evaluating Epoch:   7%|[32mâ–‹         [0m| 3/42 [00:02<00:37,  1.05it/s][A
evaluating Epoch:  10%|[32mâ–‰         [0m| 4/42 [00:03<00:35,  1.06it/s][A
evaluating Epoch:  10%|[32mâ–‰         [0m| 4/42 [00:03<00:36,  1.05it/s][A
evaluating Epoch:  12%|[32mâ–ˆâ–        [0m| 5/42 [00:04<00:35,  1.05it/s][A
evaluating Epoch:  12%|[32mâ–ˆâ–        [0m| 5/42 [00:04<00:35,  1.04it/s][A
evaluating Epoch:  14%|[32mâ–ˆâ–        [0m| 6/42 [00:05<00:34,  1.06it/s][A
evaluating Epoch:  14%|[32mâ–ˆâ–        [0m| 6/42 [00:05<00:34,  1.06it/s][A
evaluating Epoch:  17%|[32mâ–ˆâ–‹        [0m| 7/42 [00:06<00:33,  1.06it/s][A
evaluating Epoch:  17%|[32mâ–ˆâ–‹        [0m| 7/42 [00:06<00:33,  1.06it/s][A
evaluating Epoch:  19%|[32mâ–ˆâ–‰        [0m| 8/42 [00:07<00:32,  1.05it/s][A
evaluating Epoch:  19%|[32mâ–ˆâ–‰        [0m| 8/42 [00:07<00:32,  1.05it/s][A
evaluating Epoch:  21%|[32mâ–ˆâ–ˆâ–       [0m| 9/42 [00:08<00:31,  1.05it/s][A
evaluating Epoch:  21%|[32mâ–ˆâ–ˆâ–       [0m| 9/42 [00:08<00:31,  1.05it/s][A

evaluating Epoch:  24%|[32mâ–ˆâ–ˆâ–       [0m| 10/42 [00:09<00:29,  1.07it/s][Aevaluating Epoch:  24%|[32mâ–ˆâ–ˆâ–       [0m| 10/42 [00:09<00:29,  1.07it/s][A
evaluating Epoch:  26%|[32mâ–ˆâ–ˆâ–Œ       [0m| 11/42 [00:10<00:29,  1.07it/s][A
evaluating Epoch:  26%|[32mâ–ˆâ–ˆâ–Œ       [0m| 11/42 [00:10<00:29,  1.07it/s][A
evaluating Epoch:  29%|[32mâ–ˆâ–ˆâ–Š       [0m| 12/42 [00:11<00:27,  1.07it/s][A
evaluating Epoch:  29%|[32mâ–ˆâ–ˆâ–Š       [0m| 12/42 [00:11<00:27,  1.07it/s][A
evaluating Epoch:  31%|[32mâ–ˆâ–ˆâ–ˆ       [0m| 13/42 [00:12<00:27,  1.07it/s][A
evaluating Epoch:  31%|[32mâ–ˆâ–ˆâ–ˆ       [0m| 13/42 [00:12<00:27,  1.07it/s][A
evaluating Epoch:  33%|[32mâ–ˆâ–ˆâ–ˆâ–Ž      [0m| 14/42 [00:13<00:26,  1.07it/s][A
evaluating Epoch:  33%|[32mâ–ˆâ–ˆâ–ˆâ–Ž      [0m| 14/42 [00:13<00:26,  1.06it/s][A
evaluating Epoch:  36%|[32mâ–ˆâ–ˆâ–ˆâ–Œ      [0m| 15/42 [00:14<00:25,  1.07it/s][A
evaluating Epoch:  36%|[32mâ–ˆâ–ˆâ–ˆâ–Œ      [0m| 15/42 [00:14<00:25,  1.07it/s][A
evaluating Epoch:  38%|[32mâ–ˆâ–ˆâ–ˆâ–Š      [0m| 16/42 [00:15<00:24,  1.08it/s][A
evaluating Epoch:  38%|[32mâ–ˆâ–ˆâ–ˆâ–Š      [0m| 16/42 [00:15<00:24,  1.08it/s][A
evaluating Epoch:  40%|[32mâ–ˆâ–ˆâ–ˆâ–ˆ      [0m| 17/42 [00:16<00:23,  1.08it/s][A
evaluating Epoch:  40%|[32mâ–ˆâ–ˆâ–ˆâ–ˆ      [0m| 17/42 [00:16<00:23,  1.08it/s][A
evaluating Epoch:  43%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž     [0m| 18/42 [00:16<00:22,  1.08it/s][A
evaluating Epoch:  43%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž     [0m| 18/42 [00:16<00:22,  1.08it/s][A
evaluating Epoch:  45%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     [0m| 19/42 [00:17<00:21,  1.08it/s][A
evaluating Epoch:  45%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     [0m| 19/42 [00:17<00:21,  1.08it/s][A
evaluating Epoch:  48%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     [0m| 20/42 [00:18<00:20,  1.08it/s][A
evaluating Epoch:  48%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     [0m| 20/42 [00:18<00:20,  1.08it/s][A

evaluating Epoch:  50%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     [0m| 21/42 [00:19<00:19,  1.08it/s][Aevaluating Epoch:  50%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     [0m| 21/42 [00:19<00:19,  1.08it/s][A
evaluating Epoch:  52%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    [0m| 22/42 [00:20<00:18,  1.08it/s][A
evaluating Epoch:  52%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    [0m| 22/42 [00:20<00:18,  1.08it/s][A
evaluating Epoch:  55%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    [0m| 23/42 [00:21<00:17,  1.09it/s][A
evaluating Epoch:  55%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    [0m| 23/42 [00:21<00:17,  1.09it/s][A
evaluating Epoch:  57%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    [0m| 24/42 [00:22<00:16,  1.08it/s][A
evaluating Epoch:  57%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    [0m| 24/42 [00:22<00:16,  1.08it/s][A
evaluating Epoch:  60%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    [0m| 25/42 [00:23<00:15,  1.08it/s][A
evaluating Epoch:  60%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    [0m| 25/42 [00:23<00:15,  1.08it/s][A
evaluating Epoch:  62%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   [0m| 26/42 [00:24<00:14,  1.07it/s][A
evaluating Epoch:  62%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   [0m| 26/42 [00:24<00:15,  1.06it/s][A
evaluating Epoch:  64%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   [0m| 27/42 [00:25<00:14,  1.07it/s][A
evaluating Epoch:  64%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   [0m| 27/42 [00:25<00:14,  1.06it/s][A
evaluating Epoch:  67%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   [0m| 28/42 [00:26<00:13,  1.07it/s][A
evaluating Epoch:  67%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   [0m| 28/42 [00:26<00:13,  1.07it/s][A

evaluating Epoch:  69%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   [0m| 29/42 [00:27<00:12,  1.07it/s][Aevaluating Epoch:  69%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   [0m| 29/42 [00:27<00:12,  1.07it/s][A
evaluating Epoch:  71%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  [0m| 30/42 [00:28<00:11,  1.06it/s][A
evaluating Epoch:  71%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  [0m| 30/42 [00:28<00:11,  1.06it/s][A
evaluating Epoch:  74%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  [0m| 31/42 [00:29<00:10,  1.07it/s][A
evaluating Epoch:  74%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  [0m| 31/42 [00:29<00:10,  1.06it/s][A
evaluating Epoch:  76%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  [0m| 32/42 [00:30<00:09,  1.07it/s][A
evaluating Epoch:  76%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  [0m| 32/42 [00:29<00:09,  1.07it/s][A
evaluating Epoch:  79%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  [0m| 33/42 [00:30<00:08,  1.06it/s][A
evaluating Epoch:  79%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  [0m| 33/42 [00:30<00:08,  1.06it/s][A
evaluating Epoch:  81%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  [0m| 34/42 [00:31<00:07,  1.06it/s][A
evaluating Epoch:  81%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  [0m| 34/42 [00:31<00:07,  1.06it/s][A
evaluating Epoch:  83%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž [0m| 35/42 [00:32<00:06,  1.07it/s][A
evaluating Epoch:  83%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž [0m| 35/42 [00:32<00:06,  1.06it/s][A

evaluating Epoch:  86%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ [0m| 36/42 [00:33<00:05,  1.07it/s][Aevaluating Epoch:  86%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ [0m| 36/42 [00:33<00:05,  1.07it/s][A
evaluating Epoch:  88%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š [0m| 37/42 [00:34<00:04,  1.08it/s][A
evaluating Epoch:  88%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š [0m| 37/42 [00:34<00:04,  1.07it/s][A
evaluating Epoch:  90%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ [0m| 38/42 [00:35<00:03,  1.08it/s][A
evaluating Epoch:  90%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ [0m| 38/42 [00:35<00:03,  1.08it/s][AW1023 04:30:57.863039 140219395401536 torch/distributed/elastic/agent/server/api.py:688] Received Signals.SIGHUP death signal, shutting down workers
W1023 04:30:57.864621 140219395401536 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 10595 closing signal SIGHUP
W1023 04:30:57.864903 140219395401536 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 10596 closing signal SIGHUP
Traceback (most recent call last):
  File "/home/uncertainty2/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/uncertainty2/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/uncertainty2/lib/python3.9/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/uncertainty2/lib/python3.9/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/uncertainty2/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/uncertainty2/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/home/uncertainty2/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/uncertainty2/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 680, in run
    result = self._invoke_run(role)
  File "/home/uncertainty2/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 835, in _invoke_run
    time.sleep(monitor_interval)
  File "/home/uncertainty2/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 79, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 10560 got signal: 1
